# src/wrf/scheduler/download_gfs.py
import os
import requests
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed
import logging
import sys
from datetime import datetime

# === æ—¥å¿—è®¾ç½® ===
log_dir = "/home/projects/logs"
os.makedirs(log_dir, exist_ok=True)
log_path = os.path.join(log_dir, "download_gfs.log")

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_path, mode='a'),  # æ”¹ä¸ºè¿½åŠ å†™å…¥
    ]
)
logger = logging.getLogger(__name__)

# === ä¸»æ‰§è¡Œå‡½æ•° ===
def run():
    logger.info("ğŸš€ å¯åŠ¨ GFS æ•°æ®ä¸‹è½½ä»»åŠ¡...")

    # è‡ªåŠ¨è·å–ä»Šå¤©æ—¥æœŸ
    forecast_date = datetime.utcnow().strftime('%Y%m%d')
    forecast_cycle = "00"
    forecast_hours = range(0, 97, 6)

    # ä¸‹è½½åœ°å€è®¾ç½®
    base_url = f"https://noaa-gfs-bdp-pds.s3.amazonaws.com/gfs.{forecast_date}/{forecast_cycle}/atmos"

    # ç›®æ ‡ä¿å­˜ç›®å½•
    gfs_base_dir = "/WRF/Product_WRF/GFS_DATA"
    save_dir = os.path.join(gfs_base_dir, f"gfs_{forecast_date}_{forecast_cycle}z")
    os.makedirs(save_dir, exist_ok=True)
    logger.info(f"ğŸ“ GFS æ•°æ®å°†ä¿å­˜è‡³: {save_dir}")

    def download_file(hour):
        forecast_hour = f"{hour:03d}"
        file_name = f"gfs.t{forecast_cycle}z.pgrb2.0p25.f{forecast_hour}"
        file_url = f"{base_url}/{file_name}"
        save_path = os.path.join(save_dir, file_name)

        max_retries = 5
        for attempt in range(max_retries):
            try:
                if attempt > 0:
                    logger.warning(f"ç¬¬{attempt}æ¬¡é‡è¯•ä¸‹è½½ {file_name}...")
                response = requests.get(file_url, stream=True)

                if response.status_code == 200:
                    total_size = int(response.headers.get('content-length', 0))
                    with open(save_path, 'wb') as f, tqdm(
                        desc=file_name,
                        total=total_size,
                        unit='B',
                        unit_scale=True,
                        unit_divisor=1024,
                    ) as bar:
                        for chunk in response.iter_content(chunk_size=1024):
                            if chunk:
                                f.write(chunk)
                                bar.update(len(chunk))
                    logger.info(f"âœ… ä¸‹è½½å®Œæˆ: {file_name}")
                    return
                else:
                    logger.error(f"ä¸‹è½½å¤±è´¥: {file_name}, çŠ¶æ€ç : {response.status_code}")
                    if attempt == max_retries - 1:
                        raise Exception(f"HTTPé”™è¯¯: çŠ¶æ€ç  {response.status_code}")
            except Exception as e:
                logger.error(f"ä¸‹è½½å¼‚å¸¸: {file_name}, é”™è¯¯: {str(e)}, å°è¯•æ¬¡æ•°: {attempt+1}")
                if attempt == max_retries - 1:
                    raise Exception(f"æœ€ç»ˆå¤±è´¥: {str(e)}")

        raise Exception(f"æ–‡ä»¶ {file_name} è¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•° {max_retries}")

    # å¤šçº¿ç¨‹å¹¶å‘ä¸‹è½½
    error_files = []
    with ThreadPoolExecutor() as executor:
        futures = [executor.submit(download_file, hour) for hour in forecast_hours]
        for future in as_completed(futures):
            try:
                future.result()
            except Exception as e:
                error_files.append(str(e))

    if error_files:
        logger.error("âŒ ä»¥ä¸‹æ–‡ä»¶ä¸‹è½½å¤±è´¥:")
        for error in error_files:
            logger.error(f"â€¢ {error}")
        raise Exception("GFS æ•°æ®ä¸‹è½½å­˜åœ¨å¤±è´¥æ–‡ä»¶ï¼Œè¯·æ£€æŸ¥æ—¥å¿—")

    logger.info("ğŸ‰ æ‰€æœ‰ GFS æ–‡ä»¶ä¸‹è½½å®Œæˆï¼")


if __name__ == "__main__":
    run()