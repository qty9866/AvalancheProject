# home/projects/src/wrf
import os
import requests
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed
import logging
from datetime import datetime
import time

# === æ—¥å¿—è®¾ç½® ===
log_dir = "/home/projects/logs"
os.makedirs(log_dir, exist_ok=True)
log_path = os.path.join(log_dir, "download_gfs.log")

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_path, mode='a'),
    ]
)
logger = logging.getLogger(__name__)

# === ä¸»æ‰§è¡Œå‡½æ•° ===
def run():
    logger.info("ğŸš€ å¯åŠ¨ GFS æ•°æ®ä¸‹è½½ä»»åŠ¡...")

    # è‡ªåŠ¨ä½¿ç”¨ UTC æ—¥æœŸ
    forecast_date = datetime.utcnow().strftime('%Y%m%d')
    forecast_cycle = "00"
    forecast_hours = range(0, 97, 6)

    # ä¿®æ”¹åçš„ GFS ä¸‹è½½åœ°å€
    base_url = f"https://nomads.ncep.noaa.gov/pub/data/nccf/com/gfs/prod/gfs.{forecast_date}/{forecast_cycle}/atmos"

    # ç›®æ ‡ä¿å­˜ç›®å½•
    gfs_base_dir = "/WRF/Product_WRF/GFS_DATA"
    save_dir = os.path.join(gfs_base_dir, f"gfs_{forecast_date}_{forecast_cycle}z")
    os.makedirs(save_dir, exist_ok=True)
    logger.info(f"ğŸ“ GFS æ•°æ®å°†ä¿å­˜è‡³: {save_dir}")

    # ======================
    #    æ–­ç‚¹ç»­ä¼ ä¸‹è½½å‡½æ•°
    # ======================
    def download_file(hour):
        forecast_hour = f"{hour:03d}"
        file_name = f"gfs.t{forecast_cycle}z.pgrb2.0p25.f{forecast_hour}"
        file_url = f"{base_url}/{file_name}"
        save_path = os.path.join(save_dir, file_name)

        max_retries = 10

        for attempt in range(max_retries):
            try:
                # å·²ä¸‹è½½çš„å­—èŠ‚æ•°ï¼ˆç”¨äºæ–­ç‚¹ç»­ä¼ ï¼‰
                downloaded = 0
                if os.path.exists(save_path):
                    downloaded = os.path.getsize(save_path)

                # HEAD è¯·æ±‚è·å–æ–‡ä»¶æ€»å¤§å°
                head = requests.head(file_url, timeout=20)
                if head.status_code != 200:
                    raise Exception(f"HEAD è¯·æ±‚å¤±è´¥: {head.status_code}")

                total_size = int(head.headers.get("content-length", 0))

                # âœ” å·²ç»å®Œæ•´ä¸‹è½½
                if downloaded == total_size and total_size > 0:
                    logger.info(f"âš¡ å·²å­˜åœ¨ä¸”å®Œæ•´: {file_name}")
                    return

                # Range æ–­ç‚¹ç»­ä¼ å¤´
                headers = {"Range": f"bytes={downloaded}-"}

                if attempt > 0:
                    logger.warning(f"ç¬¬ {attempt} æ¬¡é‡è¯•ä¸‹è½½ {file_name}ï¼ˆæ–­ç‚¹ç»­ä¼ ï¼‰...")

                response = requests.get(
                    file_url,
                    headers=headers,
                    stream=True,
                    timeout=30
                )

                if response.status_code not in (200, 206):
                    raise Exception(f"çŠ¶æ€ç å¼‚å¸¸: {response.status_code}")

                # æœ‰éƒ¨åˆ†å·²ä¸‹è½½åˆ™è¿½åŠ å†™å…¥
                mode = "ab" if downloaded > 0 else "wb"

                with open(save_path, mode) as f, tqdm(
                    desc=file_name,
                    total=total_size,
                    initial=downloaded,
                    unit="B",
                    unit_scale=True,
                    unit_divisor=1024,
                ) as bar:
                    for chunk in response.iter_content(chunk_size=1024 * 1024):
                        if chunk:
                            f.write(chunk)
                            bar.update(len(chunk))

                # éªŒè¯æ–‡ä»¶å®Œæ•´æ€§
                final_size = os.path.getsize(save_path)
                if final_size == total_size:
                    logger.info(f"âœ… ä¸‹è½½å®Œæˆ: {file_name}")
                    return
                else:
                    raise Exception(
                        f"æ–‡ä»¶å¤§å°ä¸ä¸€è‡´ downloaded={final_size}, expected={total_size}"
                    )

            except Exception as e:
                logger.error(f"âŒ ä¸‹è½½å¼‚å¸¸: {file_name}, é”™è¯¯: {e}, å°è¯•: {attempt+1}")
                time.sleep(2)

                if attempt == max_retries - 1:
                    raise Exception(f"{file_name} æœ€ç»ˆå¤±è´¥: {e}")

        raise Exception(f"æ–‡ä»¶è¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•°: {file_name}")

    # ======================
    #      å¹¶å‘ä¸‹è½½
    # ======================
    error_files = []

    # âœ” é™ä½å¹¶å‘ â€”â€” é¿å… AWS é™æµ
    with ThreadPoolExecutor(max_workers=3) as executor:
        futures = [executor.submit(download_file, hour) for hour in forecast_hours]

        for future in as_completed(futures):
            try:
                future.result()
            except Exception as e:
                error_files.append(str(e))

    if error_files:
        logger.error("âŒ ä»¥ä¸‹æ–‡ä»¶ä¸‹è½½å¤±è´¥:")
        for error in error_files:
            logger.error(f"â€¢ {error}")
        raise Exception("GFS æ•°æ®ä¸‹è½½å­˜åœ¨å¤±è´¥æ–‡ä»¶ï¼Œè¯·æ£€æŸ¥æ—¥å¿—")

    logger.info("ğŸ‰ æ‰€æœ‰ GFS æ–‡ä»¶ä¸‹è½½å®Œæˆï¼")


if __name__ == "__main__":
    run()